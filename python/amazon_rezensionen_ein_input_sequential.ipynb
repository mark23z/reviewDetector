{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text_  labelzahl\n",
      "0                                                  text_          0\n",
      "1      Love this!  Well made, sturdy, and very comfor...          1\n",
      "2      love it, a great upgrade from the original.  I...          1\n",
      "3      This pillow saved my back. I love the look and...          1\n",
      "4      Missing information on how to use it, but it i...          1\n",
      "...                                                  ...        ...\n",
      "40428  I had read some reviews saying that this bra r...          0\n",
      "40429  I wasn't sure exactly what it would be. It is ...          1\n",
      "40430  You can wear the hood by itself, wear it with ...          0\n",
      "40431  I liked nothing about this dress. The only rea...          1\n",
      "40432  I work in the wedding industry and have to wor...          0\n",
      "\n",
      "[40433 rows x 2 columns]\n",
      "Review Eva is on her to find a way to escape her abusive mother. When she meets a handsome stranger she doesn't know is the man she wants but the man she wants is strong and handsome. She is in love with him and will do anything to get her happily ever after.\n",
      "\n",
      "I loved this book! I can't wait to see how the next book comes out!I received a free copy of this book from the author for an honest review.\n",
      "\n",
      "This book was so good. I loved it. I loved the character development. I loved the relationship between the two main characters. The romance was real, and the story flowed at a great pace. It was a fun read. I would definitely recommend this book to anyone.I received this book for an honest review.  This is my first book by this author and I was very happy to see it.  I love the characters and the plot.  I will definitely be looking for more by this author.  This is a great series and I look forward to reading more from this author\n",
      "Label 1\n",
      "Vectorized review Eva is on her to find a way to escape her abusive mother. When she meets a handsome stranger she doesn't know is the man she wants but the man she wants is strong and handsome. She is in love with him and will do anything to get her happily ever after.\n",
      "\n",
      "I loved this book! I can't wait to see how the next book comes out!I received a free copy of this book from the author for an honest review.\n",
      "\n",
      "This book was so good. I loved it. I loved the character development. I loved the relationship between the two main characters. The romance was real, and the story flowed at a great pace. It was a fun read. I would definitely recommend this book to anyone.I received this book for an honest review.  This is my first book by this author and I was very happy to see it.  I love the characters and the plot.  I will definitely be looking for more by this author.  This is a great series and I look forward to reading more from this author 1\n",
      " 381 --->  probably\n",
      " 133 --->  looking\n",
      "Vocabulary size: 10000\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/310\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6929 - binary_accuracy: 0.5168 - val_loss: 0.6928 - val_binary_accuracy: 0.5183\n",
      "Epoch 2/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6927 - binary_accuracy: 0.5258 - val_loss: 0.6927 - val_binary_accuracy: 0.5240\n",
      "Epoch 3/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6925 - binary_accuracy: 0.5327 - val_loss: 0.6926 - val_binary_accuracy: 0.5272\n",
      "Epoch 4/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6925 - binary_accuracy: 0.5374 - val_loss: 0.6925 - val_binary_accuracy: 0.5297\n",
      "Epoch 5/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6923 - binary_accuracy: 0.5384 - val_loss: 0.6924 - val_binary_accuracy: 0.5304\n",
      "Epoch 6/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6922 - binary_accuracy: 0.5411 - val_loss: 0.6923 - val_binary_accuracy: 0.5319\n",
      "Epoch 7/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6921 - binary_accuracy: 0.5435 - val_loss: 0.6922 - val_binary_accuracy: 0.5341\n",
      "Epoch 8/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6920 - binary_accuracy: 0.5451 - val_loss: 0.6921 - val_binary_accuracy: 0.5364\n",
      "Epoch 9/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6919 - binary_accuracy: 0.5447 - val_loss: 0.6919 - val_binary_accuracy: 0.5368\n",
      "Epoch 10/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6917 - binary_accuracy: 0.5469 - val_loss: 0.6918 - val_binary_accuracy: 0.5383\n",
      "Epoch 11/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6916 - binary_accuracy: 0.5476 - val_loss: 0.6917 - val_binary_accuracy: 0.5383\n",
      "Epoch 12/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6914 - binary_accuracy: 0.5484 - val_loss: 0.6916 - val_binary_accuracy: 0.5396\n",
      "Epoch 13/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6914 - binary_accuracy: 0.5491 - val_loss: 0.6915 - val_binary_accuracy: 0.5410\n",
      "Epoch 14/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6911 - binary_accuracy: 0.5490 - val_loss: 0.6914 - val_binary_accuracy: 0.5428\n",
      "Epoch 15/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6910 - binary_accuracy: 0.5511 - val_loss: 0.6912 - val_binary_accuracy: 0.5448\n",
      "Epoch 16/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6908 - binary_accuracy: 0.5527 - val_loss: 0.6911 - val_binary_accuracy: 0.5453\n",
      "Epoch 17/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6908 - binary_accuracy: 0.5524 - val_loss: 0.6910 - val_binary_accuracy: 0.5460\n",
      "Epoch 18/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6907 - binary_accuracy: 0.5529 - val_loss: 0.6909 - val_binary_accuracy: 0.5482\n",
      "Epoch 19/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6904 - binary_accuracy: 0.5544 - val_loss: 0.6907 - val_binary_accuracy: 0.5492\n",
      "Epoch 20/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6903 - binary_accuracy: 0.5544 - val_loss: 0.6906 - val_binary_accuracy: 0.5504\n",
      "Epoch 21/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6903 - binary_accuracy: 0.5539 - val_loss: 0.6905 - val_binary_accuracy: 0.5512\n",
      "Epoch 22/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6901 - binary_accuracy: 0.5549 - val_loss: 0.6903 - val_binary_accuracy: 0.5514\n",
      "Epoch 23/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6900 - binary_accuracy: 0.5568 - val_loss: 0.6902 - val_binary_accuracy: 0.5527\n",
      "Epoch 24/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6898 - binary_accuracy: 0.5574 - val_loss: 0.6901 - val_binary_accuracy: 0.5534\n",
      "Epoch 25/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6897 - binary_accuracy: 0.5584 - val_loss: 0.6899 - val_binary_accuracy: 0.5539\n",
      "Epoch 26/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6896 - binary_accuracy: 0.5594 - val_loss: 0.6898 - val_binary_accuracy: 0.5549\n",
      "Epoch 27/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6895 - binary_accuracy: 0.5607 - val_loss: 0.6896 - val_binary_accuracy: 0.5561\n",
      "Epoch 28/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6893 - binary_accuracy: 0.5608 - val_loss: 0.6895 - val_binary_accuracy: 0.5574\n",
      "Epoch 29/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6892 - binary_accuracy: 0.5620 - val_loss: 0.6893 - val_binary_accuracy: 0.5589\n",
      "Epoch 30/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6890 - binary_accuracy: 0.5643 - val_loss: 0.6892 - val_binary_accuracy: 0.5596\n",
      "Epoch 31/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6889 - binary_accuracy: 0.5657 - val_loss: 0.6890 - val_binary_accuracy: 0.5613\n",
      "Epoch 32/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6887 - binary_accuracy: 0.5666 - val_loss: 0.6889 - val_binary_accuracy: 0.5631\n",
      "Epoch 33/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6886 - binary_accuracy: 0.5674 - val_loss: 0.6887 - val_binary_accuracy: 0.5643\n",
      "Epoch 34/310\n",
      "1/1 [==============================] - 1s 995ms/step - loss: 0.6884 - binary_accuracy: 0.5702 - val_loss: 0.6885 - val_binary_accuracy: 0.5648\n",
      "Epoch 35/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6882 - binary_accuracy: 0.5705 - val_loss: 0.6884 - val_binary_accuracy: 0.5663\n",
      "Epoch 36/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6881 - binary_accuracy: 0.5739 - val_loss: 0.6882 - val_binary_accuracy: 0.5680\n",
      "Epoch 37/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6879 - binary_accuracy: 0.5740 - val_loss: 0.6881 - val_binary_accuracy: 0.5685\n",
      "Epoch 38/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6878 - binary_accuracy: 0.5757 - val_loss: 0.6879 - val_binary_accuracy: 0.5710\n",
      "Epoch 39/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6876 - binary_accuracy: 0.5767 - val_loss: 0.6877 - val_binary_accuracy: 0.5732\n",
      "Epoch 40/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6874 - binary_accuracy: 0.5797 - val_loss: 0.6875 - val_binary_accuracy: 0.5739\n",
      "Epoch 41/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6873 - binary_accuracy: 0.5811 - val_loss: 0.6874 - val_binary_accuracy: 0.5754\n",
      "Epoch 42/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6872 - binary_accuracy: 0.5828 - val_loss: 0.6872 - val_binary_accuracy: 0.5762\n",
      "Epoch 43/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6869 - binary_accuracy: 0.5842 - val_loss: 0.6870 - val_binary_accuracy: 0.5769\n",
      "Epoch 44/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6868 - binary_accuracy: 0.5870 - val_loss: 0.6868 - val_binary_accuracy: 0.5784\n",
      "Epoch 45/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6866 - binary_accuracy: 0.5880 - val_loss: 0.6867 - val_binary_accuracy: 0.5796\n",
      "Epoch 46/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6865 - binary_accuracy: 0.5891 - val_loss: 0.6865 - val_binary_accuracy: 0.5814\n",
      "Epoch 47/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6863 - binary_accuracy: 0.5901 - val_loss: 0.6863 - val_binary_accuracy: 0.5843\n",
      "Epoch 48/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6862 - binary_accuracy: 0.5912 - val_loss: 0.6861 - val_binary_accuracy: 0.5865\n",
      "Epoch 49/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6859 - binary_accuracy: 0.5937 - val_loss: 0.6859 - val_binary_accuracy: 0.5870\n",
      "Epoch 50/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6858 - binary_accuracy: 0.5943 - val_loss: 0.6858 - val_binary_accuracy: 0.5883\n",
      "Epoch 51/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6856 - binary_accuracy: 0.5962 - val_loss: 0.6856 - val_binary_accuracy: 0.5888\n",
      "Epoch 52/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6854 - binary_accuracy: 0.5971 - val_loss: 0.6854 - val_binary_accuracy: 0.5895\n",
      "Epoch 53/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6852 - binary_accuracy: 0.5991 - val_loss: 0.6852 - val_binary_accuracy: 0.5908\n",
      "Epoch 54/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6851 - binary_accuracy: 0.6004 - val_loss: 0.6850 - val_binary_accuracy: 0.5922\n",
      "Epoch 55/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6847 - binary_accuracy: 0.5996 - val_loss: 0.6848 - val_binary_accuracy: 0.5935\n",
      "Epoch 56/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6847 - binary_accuracy: 0.6029 - val_loss: 0.6846 - val_binary_accuracy: 0.5947\n",
      "Epoch 57/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6844 - binary_accuracy: 0.6022 - val_loss: 0.6844 - val_binary_accuracy: 0.5950\n",
      "Epoch 58/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6843 - binary_accuracy: 0.6031 - val_loss: 0.6843 - val_binary_accuracy: 0.5962\n",
      "Epoch 59/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6840 - binary_accuracy: 0.6040 - val_loss: 0.6841 - val_binary_accuracy: 0.5974\n",
      "Epoch 60/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6839 - binary_accuracy: 0.6070 - val_loss: 0.6839 - val_binary_accuracy: 0.5974\n",
      "Epoch 61/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6836 - binary_accuracy: 0.6060 - val_loss: 0.6837 - val_binary_accuracy: 0.5989\n",
      "Epoch 62/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6834 - binary_accuracy: 0.6082 - val_loss: 0.6834 - val_binary_accuracy: 0.5999\n",
      "Epoch 63/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6832 - binary_accuracy: 0.6079 - val_loss: 0.6832 - val_binary_accuracy: 0.6014\n",
      "Epoch 64/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6831 - binary_accuracy: 0.6096 - val_loss: 0.6830 - val_binary_accuracy: 0.6026\n",
      "Epoch 65/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6829 - binary_accuracy: 0.6111 - val_loss: 0.6828 - val_binary_accuracy: 0.6034\n",
      "Epoch 66/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6826 - binary_accuracy: 0.6119 - val_loss: 0.6826 - val_binary_accuracy: 0.6056\n",
      "Epoch 67/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6824 - binary_accuracy: 0.6145 - val_loss: 0.6824 - val_binary_accuracy: 0.6058\n",
      "Epoch 68/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6822 - binary_accuracy: 0.6145 - val_loss: 0.6822 - val_binary_accuracy: 0.6073\n",
      "Epoch 69/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6819 - binary_accuracy: 0.6145 - val_loss: 0.6820 - val_binary_accuracy: 0.6078\n",
      "Epoch 70/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6818 - binary_accuracy: 0.6187 - val_loss: 0.6818 - val_binary_accuracy: 0.6093\n",
      "Epoch 71/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6816 - binary_accuracy: 0.6175 - val_loss: 0.6815 - val_binary_accuracy: 0.6115\n",
      "Epoch 72/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6814 - binary_accuracy: 0.6200 - val_loss: 0.6813 - val_binary_accuracy: 0.6135\n",
      "Epoch 73/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6810 - binary_accuracy: 0.6220 - val_loss: 0.6811 - val_binary_accuracy: 0.6147\n",
      "Epoch 74/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6809 - binary_accuracy: 0.6219 - val_loss: 0.6809 - val_binary_accuracy: 0.6160\n",
      "Epoch 75/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6808 - binary_accuracy: 0.6225 - val_loss: 0.6806 - val_binary_accuracy: 0.6184\n",
      "Epoch 76/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6804 - binary_accuracy: 0.6253 - val_loss: 0.6804 - val_binary_accuracy: 0.6189\n",
      "Epoch 77/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6802 - binary_accuracy: 0.6265 - val_loss: 0.6802 - val_binary_accuracy: 0.6209\n",
      "Epoch 78/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6801 - binary_accuracy: 0.6293 - val_loss: 0.6799 - val_binary_accuracy: 0.6219\n",
      "Epoch 79/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6797 - binary_accuracy: 0.6297 - val_loss: 0.6797 - val_binary_accuracy: 0.6222\n",
      "Epoch 80/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6796 - binary_accuracy: 0.6318 - val_loss: 0.6794 - val_binary_accuracy: 0.6236\n",
      "Epoch 81/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6792 - binary_accuracy: 0.6328 - val_loss: 0.6792 - val_binary_accuracy: 0.6251\n",
      "Epoch 82/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6790 - binary_accuracy: 0.6344 - val_loss: 0.6790 - val_binary_accuracy: 0.6269\n",
      "Epoch 83/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6789 - binary_accuracy: 0.6355 - val_loss: 0.6787 - val_binary_accuracy: 0.6273\n",
      "Epoch 84/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6785 - binary_accuracy: 0.6373 - val_loss: 0.6785 - val_binary_accuracy: 0.6278\n",
      "Epoch 85/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6783 - binary_accuracy: 0.6372 - val_loss: 0.6782 - val_binary_accuracy: 0.6286\n",
      "Epoch 86/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6781 - binary_accuracy: 0.6379 - val_loss: 0.6780 - val_binary_accuracy: 0.6296\n",
      "Epoch 87/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6779 - binary_accuracy: 0.6380 - val_loss: 0.6777 - val_binary_accuracy: 0.6308\n",
      "Epoch 88/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6775 - binary_accuracy: 0.6407 - val_loss: 0.6775 - val_binary_accuracy: 0.6316\n",
      "Epoch 89/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6774 - binary_accuracy: 0.6416 - val_loss: 0.6772 - val_binary_accuracy: 0.6325\n",
      "Epoch 90/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6772 - binary_accuracy: 0.6429 - val_loss: 0.6770 - val_binary_accuracy: 0.6330\n",
      "Epoch 91/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6769 - binary_accuracy: 0.6427 - val_loss: 0.6767 - val_binary_accuracy: 0.6343\n",
      "Epoch 92/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6765 - binary_accuracy: 0.6454 - val_loss: 0.6765 - val_binary_accuracy: 0.6348\n",
      "Epoch 93/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6764 - binary_accuracy: 0.6449 - val_loss: 0.6762 - val_binary_accuracy: 0.6367\n",
      "Epoch 94/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6759 - binary_accuracy: 0.6473 - val_loss: 0.6759 - val_binary_accuracy: 0.6380\n",
      "Epoch 95/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6758 - binary_accuracy: 0.6496 - val_loss: 0.6757 - val_binary_accuracy: 0.6392\n",
      "Epoch 96/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6755 - binary_accuracy: 0.6487 - val_loss: 0.6754 - val_binary_accuracy: 0.6407\n",
      "Epoch 97/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6752 - binary_accuracy: 0.6488 - val_loss: 0.6751 - val_binary_accuracy: 0.6412\n",
      "Epoch 98/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6750 - binary_accuracy: 0.6498 - val_loss: 0.6749 - val_binary_accuracy: 0.6422\n",
      "Epoch 99/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6747 - binary_accuracy: 0.6513 - val_loss: 0.6746 - val_binary_accuracy: 0.6434\n",
      "Epoch 100/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6745 - binary_accuracy: 0.6530 - val_loss: 0.6743 - val_binary_accuracy: 0.6447\n",
      "Epoch 101/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6742 - binary_accuracy: 0.6532 - val_loss: 0.6740 - val_binary_accuracy: 0.6461\n",
      "Epoch 102/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6739 - binary_accuracy: 0.6549 - val_loss: 0.6738 - val_binary_accuracy: 0.6474\n",
      "Epoch 103/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6736 - binary_accuracy: 0.6557 - val_loss: 0.6735 - val_binary_accuracy: 0.6484\n",
      "Epoch 104/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6733 - binary_accuracy: 0.6589 - val_loss: 0.6732 - val_binary_accuracy: 0.6496\n",
      "Epoch 105/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6731 - binary_accuracy: 0.6615 - val_loss: 0.6729 - val_binary_accuracy: 0.6513\n",
      "Epoch 106/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6727 - binary_accuracy: 0.6605 - val_loss: 0.6726 - val_binary_accuracy: 0.6526\n",
      "Epoch 107/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6725 - binary_accuracy: 0.6612 - val_loss: 0.6723 - val_binary_accuracy: 0.6533\n",
      "Epoch 108/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6722 - binary_accuracy: 0.6631 - val_loss: 0.6720 - val_binary_accuracy: 0.6543\n",
      "Epoch 109/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6718 - binary_accuracy: 0.6653 - val_loss: 0.6717 - val_binary_accuracy: 0.6548\n",
      "Epoch 110/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6716 - binary_accuracy: 0.6653 - val_loss: 0.6715 - val_binary_accuracy: 0.6550\n",
      "Epoch 111/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6713 - binary_accuracy: 0.6658 - val_loss: 0.6712 - val_binary_accuracy: 0.6558\n",
      "Epoch 112/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6710 - binary_accuracy: 0.6664 - val_loss: 0.6709 - val_binary_accuracy: 0.6575\n",
      "Epoch 113/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6707 - binary_accuracy: 0.6674 - val_loss: 0.6706 - val_binary_accuracy: 0.6583\n",
      "Epoch 114/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6704 - binary_accuracy: 0.6690 - val_loss: 0.6703 - val_binary_accuracy: 0.6583\n",
      "Epoch 115/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6701 - binary_accuracy: 0.6682 - val_loss: 0.6700 - val_binary_accuracy: 0.6590\n",
      "Epoch 116/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6699 - binary_accuracy: 0.6705 - val_loss: 0.6697 - val_binary_accuracy: 0.6597\n",
      "Epoch 117/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6695 - binary_accuracy: 0.6721 - val_loss: 0.6694 - val_binary_accuracy: 0.6597\n",
      "Epoch 118/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6692 - binary_accuracy: 0.6737 - val_loss: 0.6691 - val_binary_accuracy: 0.6612\n",
      "Epoch 119/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6689 - binary_accuracy: 0.6736 - val_loss: 0.6687 - val_binary_accuracy: 0.6622\n",
      "Epoch 120/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6685 - binary_accuracy: 0.6745 - val_loss: 0.6684 - val_binary_accuracy: 0.6625\n",
      "Epoch 121/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6684 - binary_accuracy: 0.6760 - val_loss: 0.6681 - val_binary_accuracy: 0.6632\n",
      "Epoch 122/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6680 - binary_accuracy: 0.6790 - val_loss: 0.6678 - val_binary_accuracy: 0.6642\n",
      "Epoch 123/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6676 - binary_accuracy: 0.6797 - val_loss: 0.6675 - val_binary_accuracy: 0.6659\n",
      "Epoch 124/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6673 - binary_accuracy: 0.6819 - val_loss: 0.6672 - val_binary_accuracy: 0.6684\n",
      "Epoch 125/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6670 - binary_accuracy: 0.6814 - val_loss: 0.6668 - val_binary_accuracy: 0.6699\n",
      "Epoch 126/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6668 - binary_accuracy: 0.6839 - val_loss: 0.6665 - val_binary_accuracy: 0.6711\n",
      "Epoch 127/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6664 - binary_accuracy: 0.6841 - val_loss: 0.6662 - val_binary_accuracy: 0.6719\n",
      "Epoch 128/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6659 - binary_accuracy: 0.6851 - val_loss: 0.6659 - val_binary_accuracy: 0.6726\n",
      "Epoch 129/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6660 - binary_accuracy: 0.6849 - val_loss: 0.6656 - val_binary_accuracy: 0.6748\n",
      "Epoch 130/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6655 - binary_accuracy: 0.6860 - val_loss: 0.6652 - val_binary_accuracy: 0.6771\n",
      "Epoch 131/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6650 - binary_accuracy: 0.6891 - val_loss: 0.6649 - val_binary_accuracy: 0.6788\n",
      "Epoch 132/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6649 - binary_accuracy: 0.6902 - val_loss: 0.6646 - val_binary_accuracy: 0.6800\n",
      "Epoch 133/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6644 - binary_accuracy: 0.6931 - val_loss: 0.6642 - val_binary_accuracy: 0.6808\n",
      "Epoch 134/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6641 - binary_accuracy: 0.6922 - val_loss: 0.6639 - val_binary_accuracy: 0.6818\n",
      "Epoch 135/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6638 - binary_accuracy: 0.6929 - val_loss: 0.6636 - val_binary_accuracy: 0.6820\n",
      "Epoch 136/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6634 - binary_accuracy: 0.6958 - val_loss: 0.6632 - val_binary_accuracy: 0.6830\n",
      "Epoch 137/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6632 - binary_accuracy: 0.6963 - val_loss: 0.6629 - val_binary_accuracy: 0.6840\n",
      "Epoch 138/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6627 - binary_accuracy: 0.6983 - val_loss: 0.6625 - val_binary_accuracy: 0.6845\n",
      "Epoch 139/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6624 - binary_accuracy: 0.6981 - val_loss: 0.6622 - val_binary_accuracy: 0.6847\n",
      "Epoch 140/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6620 - binary_accuracy: 0.6975 - val_loss: 0.6619 - val_binary_accuracy: 0.6852\n",
      "Epoch 141/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6619 - binary_accuracy: 0.6957 - val_loss: 0.6615 - val_binary_accuracy: 0.6867\n",
      "Epoch 142/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6613 - binary_accuracy: 0.6987 - val_loss: 0.6612 - val_binary_accuracy: 0.6882\n",
      "Epoch 143/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6609 - binary_accuracy: 0.6997 - val_loss: 0.6608 - val_binary_accuracy: 0.6889\n",
      "Epoch 144/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6608 - binary_accuracy: 0.6986 - val_loss: 0.6605 - val_binary_accuracy: 0.6894\n",
      "Epoch 145/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6603 - binary_accuracy: 0.7035 - val_loss: 0.6601 - val_binary_accuracy: 0.6909\n",
      "Epoch 146/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6600 - binary_accuracy: 0.7031 - val_loss: 0.6598 - val_binary_accuracy: 0.6916\n",
      "Epoch 147/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6598 - binary_accuracy: 0.7024 - val_loss: 0.6594 - val_binary_accuracy: 0.6926\n",
      "Epoch 148/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6593 - binary_accuracy: 0.7065 - val_loss: 0.6591 - val_binary_accuracy: 0.6939\n",
      "Epoch 149/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6590 - binary_accuracy: 0.7055 - val_loss: 0.6587 - val_binary_accuracy: 0.6946\n",
      "Epoch 150/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6586 - binary_accuracy: 0.7087 - val_loss: 0.6583 - val_binary_accuracy: 0.6968\n",
      "Epoch 151/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6582 - binary_accuracy: 0.7112 - val_loss: 0.6580 - val_binary_accuracy: 0.6978\n",
      "Epoch 152/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6579 - binary_accuracy: 0.7133 - val_loss: 0.6576 - val_binary_accuracy: 0.6996\n",
      "Epoch 153/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6574 - binary_accuracy: 0.7141 - val_loss: 0.6572 - val_binary_accuracy: 0.7005\n",
      "Epoch 154/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6572 - binary_accuracy: 0.7138 - val_loss: 0.6569 - val_binary_accuracy: 0.7015\n",
      "Epoch 155/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6568 - binary_accuracy: 0.7128 - val_loss: 0.6565 - val_binary_accuracy: 0.7018\n",
      "Epoch 156/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6564 - binary_accuracy: 0.7147 - val_loss: 0.6561 - val_binary_accuracy: 0.7023\n",
      "Epoch 157/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6562 - binary_accuracy: 0.7172 - val_loss: 0.6558 - val_binary_accuracy: 0.7030\n",
      "Epoch 158/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6557 - binary_accuracy: 0.7184 - val_loss: 0.6554 - val_binary_accuracy: 0.7035\n",
      "Epoch 159/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6553 - binary_accuracy: 0.7162 - val_loss: 0.6550 - val_binary_accuracy: 0.7030\n",
      "Epoch 160/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6549 - binary_accuracy: 0.7192 - val_loss: 0.6547 - val_binary_accuracy: 0.7043\n",
      "Epoch 161/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6546 - binary_accuracy: 0.7202 - val_loss: 0.6543 - val_binary_accuracy: 0.7060\n",
      "Epoch 162/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6542 - binary_accuracy: 0.7220 - val_loss: 0.6539 - val_binary_accuracy: 0.7075\n",
      "Epoch 163/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6539 - binary_accuracy: 0.7232 - val_loss: 0.6535 - val_binary_accuracy: 0.7090\n",
      "Epoch 164/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6535 - binary_accuracy: 0.7236 - val_loss: 0.6531 - val_binary_accuracy: 0.7104\n",
      "Epoch 165/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6532 - binary_accuracy: 0.7245 - val_loss: 0.6528 - val_binary_accuracy: 0.7117\n",
      "Epoch 166/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6526 - binary_accuracy: 0.7263 - val_loss: 0.6524 - val_binary_accuracy: 0.7119\n",
      "Epoch 167/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6523 - binary_accuracy: 0.7279 - val_loss: 0.6520 - val_binary_accuracy: 0.7119\n",
      "Epoch 168/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6518 - binary_accuracy: 0.7283 - val_loss: 0.6516 - val_binary_accuracy: 0.7119\n",
      "Epoch 169/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6513 - binary_accuracy: 0.7297 - val_loss: 0.6512 - val_binary_accuracy: 0.7122\n",
      "Epoch 170/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6510 - binary_accuracy: 0.7284 - val_loss: 0.6509 - val_binary_accuracy: 0.7119\n",
      "Epoch 171/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6507 - binary_accuracy: 0.7280 - val_loss: 0.6505 - val_binary_accuracy: 0.7117\n",
      "Epoch 172/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6505 - binary_accuracy: 0.7265 - val_loss: 0.6501 - val_binary_accuracy: 0.7119\n",
      "Epoch 173/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6498 - binary_accuracy: 0.7282 - val_loss: 0.6497 - val_binary_accuracy: 0.7132\n",
      "Epoch 174/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6495 - binary_accuracy: 0.7288 - val_loss: 0.6493 - val_binary_accuracy: 0.7151\n",
      "Epoch 175/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6492 - binary_accuracy: 0.7307 - val_loss: 0.6489 - val_binary_accuracy: 0.7156\n",
      "Epoch 176/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6488 - binary_accuracy: 0.7329 - val_loss: 0.6485 - val_binary_accuracy: 0.7181\n",
      "Epoch 177/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6483 - binary_accuracy: 0.7335 - val_loss: 0.6481 - val_binary_accuracy: 0.7198\n",
      "Epoch 178/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6479 - binary_accuracy: 0.7376 - val_loss: 0.6477 - val_binary_accuracy: 0.7228\n",
      "Epoch 179/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6475 - binary_accuracy: 0.7392 - val_loss: 0.6473 - val_binary_accuracy: 0.7255\n",
      "Epoch 180/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6472 - binary_accuracy: 0.7376 - val_loss: 0.6469 - val_binary_accuracy: 0.7277\n",
      "Epoch 181/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6468 - binary_accuracy: 0.7400 - val_loss: 0.6465 - val_binary_accuracy: 0.7287\n",
      "Epoch 182/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6465 - binary_accuracy: 0.7426 - val_loss: 0.6461 - val_binary_accuracy: 0.7302\n",
      "Epoch 183/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6460 - binary_accuracy: 0.7448 - val_loss: 0.6457 - val_binary_accuracy: 0.7302\n",
      "Epoch 184/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6457 - binary_accuracy: 0.7426 - val_loss: 0.6453 - val_binary_accuracy: 0.7300\n",
      "Epoch 185/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6452 - binary_accuracy: 0.7431 - val_loss: 0.6449 - val_binary_accuracy: 0.7305\n",
      "Epoch 186/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6447 - binary_accuracy: 0.7458 - val_loss: 0.6445 - val_binary_accuracy: 0.7315\n",
      "Epoch 187/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6441 - binary_accuracy: 0.7450 - val_loss: 0.6441 - val_binary_accuracy: 0.7315\n",
      "Epoch 188/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6440 - binary_accuracy: 0.7464 - val_loss: 0.6437 - val_binary_accuracy: 0.7327\n",
      "Epoch 189/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6436 - binary_accuracy: 0.7478 - val_loss: 0.6433 - val_binary_accuracy: 0.7347\n",
      "Epoch 190/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6432 - binary_accuracy: 0.7452 - val_loss: 0.6429 - val_binary_accuracy: 0.7364\n",
      "Epoch 191/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6426 - binary_accuracy: 0.7493 - val_loss: 0.6424 - val_binary_accuracy: 0.7384\n",
      "Epoch 192/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6424 - binary_accuracy: 0.7474 - val_loss: 0.6420 - val_binary_accuracy: 0.7406\n",
      "Epoch 193/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6419 - binary_accuracy: 0.7498 - val_loss: 0.6416 - val_binary_accuracy: 0.7418\n",
      "Epoch 194/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6414 - binary_accuracy: 0.7513 - val_loss: 0.6412 - val_binary_accuracy: 0.7433\n",
      "Epoch 195/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6410 - binary_accuracy: 0.7520 - val_loss: 0.6408 - val_binary_accuracy: 0.7433\n",
      "Epoch 196/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6406 - binary_accuracy: 0.7536 - val_loss: 0.6404 - val_binary_accuracy: 0.7448\n",
      "Epoch 197/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6402 - binary_accuracy: 0.7546 - val_loss: 0.6399 - val_binary_accuracy: 0.7460\n",
      "Epoch 198/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6399 - binary_accuracy: 0.7576 - val_loss: 0.6395 - val_binary_accuracy: 0.7485\n",
      "Epoch 199/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6395 - binary_accuracy: 0.7569 - val_loss: 0.6391 - val_binary_accuracy: 0.7498\n",
      "Epoch 200/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6388 - binary_accuracy: 0.7574 - val_loss: 0.6387 - val_binary_accuracy: 0.7515\n",
      "Epoch 201/310\n",
      "1/1 [==============================] - 1s 998ms/step - loss: 0.6385 - binary_accuracy: 0.7598 - val_loss: 0.6383 - val_binary_accuracy: 0.7520\n",
      "Epoch 202/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6383 - binary_accuracy: 0.7577 - val_loss: 0.6378 - val_binary_accuracy: 0.7532\n",
      "Epoch 203/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6376 - binary_accuracy: 0.7628 - val_loss: 0.6374 - val_binary_accuracy: 0.7537\n",
      "Epoch 204/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6374 - binary_accuracy: 0.7612 - val_loss: 0.6370 - val_binary_accuracy: 0.7537\n",
      "Epoch 205/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6368 - binary_accuracy: 0.7618 - val_loss: 0.6366 - val_binary_accuracy: 0.7537\n",
      "Epoch 206/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6363 - binary_accuracy: 0.7630 - val_loss: 0.6362 - val_binary_accuracy: 0.7537\n",
      "Epoch 207/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6358 - binary_accuracy: 0.7628 - val_loss: 0.6357 - val_binary_accuracy: 0.7537\n",
      "Epoch 208/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6357 - binary_accuracy: 0.7633 - val_loss: 0.6353 - val_binary_accuracy: 0.7547\n",
      "Epoch 209/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6354 - binary_accuracy: 0.7614 - val_loss: 0.6349 - val_binary_accuracy: 0.7554\n",
      "Epoch 210/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6346 - binary_accuracy: 0.7628 - val_loss: 0.6344 - val_binary_accuracy: 0.7562\n",
      "Epoch 211/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6342 - binary_accuracy: 0.7665 - val_loss: 0.6340 - val_binary_accuracy: 0.7572\n",
      "Epoch 212/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6339 - binary_accuracy: 0.7656 - val_loss: 0.6336 - val_binary_accuracy: 0.7584\n",
      "Epoch 213/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6333 - binary_accuracy: 0.7667 - val_loss: 0.6331 - val_binary_accuracy: 0.7596\n",
      "Epoch 214/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6332 - binary_accuracy: 0.7694 - val_loss: 0.6327 - val_binary_accuracy: 0.7609\n",
      "Epoch 215/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6325 - binary_accuracy: 0.7692 - val_loss: 0.6323 - val_binary_accuracy: 0.7616\n",
      "Epoch 216/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6320 - binary_accuracy: 0.7694 - val_loss: 0.6318 - val_binary_accuracy: 0.7621\n",
      "Epoch 217/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6317 - binary_accuracy: 0.7700 - val_loss: 0.6314 - val_binary_accuracy: 0.7641\n",
      "Epoch 218/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6311 - binary_accuracy: 0.7722 - val_loss: 0.6310 - val_binary_accuracy: 0.7648\n",
      "Epoch 219/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6308 - binary_accuracy: 0.7722 - val_loss: 0.6305 - val_binary_accuracy: 0.7656\n",
      "Epoch 220/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6306 - binary_accuracy: 0.7719 - val_loss: 0.6301 - val_binary_accuracy: 0.7690\n",
      "Epoch 221/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6300 - binary_accuracy: 0.7735 - val_loss: 0.6297 - val_binary_accuracy: 0.7695\n",
      "Epoch 222/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6296 - binary_accuracy: 0.7736 - val_loss: 0.6292 - val_binary_accuracy: 0.7718\n",
      "Epoch 223/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6292 - binary_accuracy: 0.7775 - val_loss: 0.6288 - val_binary_accuracy: 0.7732\n",
      "Epoch 224/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6287 - binary_accuracy: 0.7761 - val_loss: 0.6283 - val_binary_accuracy: 0.7747\n",
      "Epoch 225/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6283 - binary_accuracy: 0.7792 - val_loss: 0.6279 - val_binary_accuracy: 0.7752\n",
      "Epoch 226/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6278 - binary_accuracy: 0.7798 - val_loss: 0.6274 - val_binary_accuracy: 0.7760\n",
      "Epoch 227/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6274 - binary_accuracy: 0.7806 - val_loss: 0.6270 - val_binary_accuracy: 0.7757\n",
      "Epoch 228/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6267 - binary_accuracy: 0.7836 - val_loss: 0.6266 - val_binary_accuracy: 0.7757\n",
      "Epoch 229/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6266 - binary_accuracy: 0.7801 - val_loss: 0.6261 - val_binary_accuracy: 0.7750\n",
      "Epoch 230/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6259 - binary_accuracy: 0.7828 - val_loss: 0.6257 - val_binary_accuracy: 0.7752\n",
      "Epoch 231/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6258 - binary_accuracy: 0.7788 - val_loss: 0.6252 - val_binary_accuracy: 0.7752\n",
      "Epoch 232/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6249 - binary_accuracy: 0.7791 - val_loss: 0.6248 - val_binary_accuracy: 0.7752\n",
      "Epoch 233/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6245 - binary_accuracy: 0.7801 - val_loss: 0.6244 - val_binary_accuracy: 0.7755\n",
      "Epoch 234/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6242 - binary_accuracy: 0.7799 - val_loss: 0.6239 - val_binary_accuracy: 0.7755\n",
      "Epoch 235/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6237 - binary_accuracy: 0.7822 - val_loss: 0.6235 - val_binary_accuracy: 0.7762\n",
      "Epoch 236/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6233 - binary_accuracy: 0.7843 - val_loss: 0.6230 - val_binary_accuracy: 0.7782\n",
      "Epoch 237/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6229 - binary_accuracy: 0.7841 - val_loss: 0.6226 - val_binary_accuracy: 0.7789\n",
      "Epoch 238/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6223 - binary_accuracy: 0.7856 - val_loss: 0.6221 - val_binary_accuracy: 0.7799\n",
      "Epoch 239/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6221 - binary_accuracy: 0.7859 - val_loss: 0.6217 - val_binary_accuracy: 0.7821\n",
      "Epoch 240/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6214 - binary_accuracy: 0.7883 - val_loss: 0.6212 - val_binary_accuracy: 0.7839\n",
      "Epoch 241/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6211 - binary_accuracy: 0.7883 - val_loss: 0.6208 - val_binary_accuracy: 0.7844\n",
      "Epoch 242/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6206 - binary_accuracy: 0.7906 - val_loss: 0.6203 - val_binary_accuracy: 0.7849\n",
      "Epoch 243/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6202 - binary_accuracy: 0.7911 - val_loss: 0.6198 - val_binary_accuracy: 0.7854\n",
      "Epoch 244/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6196 - binary_accuracy: 0.7905 - val_loss: 0.6194 - val_binary_accuracy: 0.7854\n",
      "Epoch 245/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6192 - binary_accuracy: 0.7930 - val_loss: 0.6190 - val_binary_accuracy: 0.7854\n",
      "Epoch 246/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6187 - binary_accuracy: 0.7948 - val_loss: 0.6185 - val_binary_accuracy: 0.7844\n",
      "Epoch 247/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6184 - binary_accuracy: 0.7901 - val_loss: 0.6181 - val_binary_accuracy: 0.7841\n",
      "Epoch 248/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6178 - binary_accuracy: 0.7909 - val_loss: 0.6176 - val_binary_accuracy: 0.7839\n",
      "Epoch 249/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6173 - binary_accuracy: 0.7923 - val_loss: 0.6172 - val_binary_accuracy: 0.7841\n",
      "Epoch 250/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6170 - binary_accuracy: 0.7919 - val_loss: 0.6167 - val_binary_accuracy: 0.7846\n",
      "Epoch 251/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6166 - binary_accuracy: 0.7911 - val_loss: 0.6162 - val_binary_accuracy: 0.7859\n",
      "Epoch 252/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6162 - binary_accuracy: 0.7918 - val_loss: 0.6158 - val_binary_accuracy: 0.7881\n",
      "Epoch 253/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6156 - binary_accuracy: 0.7937 - val_loss: 0.6153 - val_binary_accuracy: 0.7925\n",
      "Epoch 254/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6150 - binary_accuracy: 0.7952 - val_loss: 0.6149 - val_binary_accuracy: 0.7948\n",
      "Epoch 255/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6146 - binary_accuracy: 0.7994 - val_loss: 0.6144 - val_binary_accuracy: 0.7960\n",
      "Epoch 256/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6143 - binary_accuracy: 0.7969 - val_loss: 0.6139 - val_binary_accuracy: 0.7962\n",
      "Epoch 257/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6138 - binary_accuracy: 0.7998 - val_loss: 0.6135 - val_binary_accuracy: 0.7962\n",
      "Epoch 258/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6134 - binary_accuracy: 0.7995 - val_loss: 0.6130 - val_binary_accuracy: 0.7960\n",
      "Epoch 259/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6126 - binary_accuracy: 0.7994 - val_loss: 0.6126 - val_binary_accuracy: 0.7955\n",
      "Epoch 260/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6124 - binary_accuracy: 0.7999 - val_loss: 0.6121 - val_binary_accuracy: 0.7943\n",
      "Epoch 261/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6120 - binary_accuracy: 0.7989 - val_loss: 0.6117 - val_binary_accuracy: 0.7938\n",
      "Epoch 262/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6114 - binary_accuracy: 0.7979 - val_loss: 0.6112 - val_binary_accuracy: 0.7938\n",
      "Epoch 263/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6111 - binary_accuracy: 0.7974 - val_loss: 0.6108 - val_binary_accuracy: 0.7948\n",
      "Epoch 264/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6106 - binary_accuracy: 0.8008 - val_loss: 0.6103 - val_binary_accuracy: 0.7960\n",
      "Epoch 265/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6102 - binary_accuracy: 0.7993 - val_loss: 0.6098 - val_binary_accuracy: 0.7965\n",
      "Epoch 266/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6099 - binary_accuracy: 0.8009 - val_loss: 0.6094 - val_binary_accuracy: 0.7980\n",
      "Epoch 267/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6095 - binary_accuracy: 0.8028 - val_loss: 0.6089 - val_binary_accuracy: 0.8009\n",
      "Epoch 268/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6090 - binary_accuracy: 0.8041 - val_loss: 0.6084 - val_binary_accuracy: 0.8034\n",
      "Epoch 269/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6084 - binary_accuracy: 0.8048 - val_loss: 0.6080 - val_binary_accuracy: 0.8061\n",
      "Epoch 270/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6077 - binary_accuracy: 0.8079 - val_loss: 0.6075 - val_binary_accuracy: 0.8066\n",
      "Epoch 271/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6073 - binary_accuracy: 0.8070 - val_loss: 0.6071 - val_binary_accuracy: 0.8056\n",
      "Epoch 272/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6069 - binary_accuracy: 0.8073 - val_loss: 0.6066 - val_binary_accuracy: 0.8046\n",
      "Epoch 273/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6064 - binary_accuracy: 0.8086 - val_loss: 0.6062 - val_binary_accuracy: 0.8034\n",
      "Epoch 274/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6061 - binary_accuracy: 0.8052 - val_loss: 0.6057 - val_binary_accuracy: 0.8027\n",
      "Epoch 275/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6056 - binary_accuracy: 0.8051 - val_loss: 0.6052 - val_binary_accuracy: 0.8027\n",
      "Epoch 276/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6051 - binary_accuracy: 0.8065 - val_loss: 0.6048 - val_binary_accuracy: 0.8029\n",
      "Epoch 277/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6048 - binary_accuracy: 0.8039 - val_loss: 0.6043 - val_binary_accuracy: 0.8037\n",
      "Epoch 278/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6041 - binary_accuracy: 0.8069 - val_loss: 0.6039 - val_binary_accuracy: 0.8054\n",
      "Epoch 279/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6039 - binary_accuracy: 0.8074 - val_loss: 0.6034 - val_binary_accuracy: 0.8076\n",
      "Epoch 280/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6035 - binary_accuracy: 0.8108 - val_loss: 0.6029 - val_binary_accuracy: 0.8084\n",
      "Epoch 281/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6029 - binary_accuracy: 0.8113 - val_loss: 0.6025 - val_binary_accuracy: 0.8106\n",
      "Epoch 282/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6022 - binary_accuracy: 0.8126 - val_loss: 0.6020 - val_binary_accuracy: 0.8111\n",
      "Epoch 283/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6019 - binary_accuracy: 0.8120 - val_loss: 0.6015 - val_binary_accuracy: 0.8111\n",
      "Epoch 284/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6014 - binary_accuracy: 0.8132 - val_loss: 0.6011 - val_binary_accuracy: 0.8103\n",
      "Epoch 285/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6007 - binary_accuracy: 0.8120 - val_loss: 0.6006 - val_binary_accuracy: 0.8089\n",
      "Epoch 286/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6005 - binary_accuracy: 0.8129 - val_loss: 0.6002 - val_binary_accuracy: 0.8089\n",
      "Epoch 287/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6002 - binary_accuracy: 0.8122 - val_loss: 0.5997 - val_binary_accuracy: 0.8089\n",
      "Epoch 288/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5994 - binary_accuracy: 0.8124 - val_loss: 0.5993 - val_binary_accuracy: 0.8089\n",
      "Epoch 289/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5993 - binary_accuracy: 0.8118 - val_loss: 0.5988 - val_binary_accuracy: 0.8091\n",
      "Epoch 290/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5988 - binary_accuracy: 0.8119 - val_loss: 0.5983 - val_binary_accuracy: 0.8106\n",
      "Epoch 291/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5981 - binary_accuracy: 0.8132 - val_loss: 0.5979 - val_binary_accuracy: 0.8123\n",
      "Epoch 292/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5974 - binary_accuracy: 0.8147 - val_loss: 0.5974 - val_binary_accuracy: 0.8138\n",
      "Epoch 293/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5972 - binary_accuracy: 0.8174 - val_loss: 0.5969 - val_binary_accuracy: 0.8145\n",
      "Epoch 294/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5968 - binary_accuracy: 0.8157 - val_loss: 0.5965 - val_binary_accuracy: 0.8160\n",
      "Epoch 295/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5963 - binary_accuracy: 0.8153 - val_loss: 0.5960 - val_binary_accuracy: 0.8165\n",
      "Epoch 296/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5959 - binary_accuracy: 0.8160 - val_loss: 0.5955 - val_binary_accuracy: 0.8168\n",
      "Epoch 297/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5953 - binary_accuracy: 0.8161 - val_loss: 0.5951 - val_binary_accuracy: 0.8165\n",
      "Epoch 298/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5950 - binary_accuracy: 0.8164 - val_loss: 0.5946 - val_binary_accuracy: 0.8168\n",
      "Epoch 299/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5945 - binary_accuracy: 0.8167 - val_loss: 0.5942 - val_binary_accuracy: 0.8170\n",
      "Epoch 300/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5940 - binary_accuracy: 0.8185 - val_loss: 0.5937 - val_binary_accuracy: 0.8173\n",
      "Epoch 301/310\n",
      "1/1 [==============================] - 1s 997ms/step - loss: 0.5936 - binary_accuracy: 0.8189 - val_loss: 0.5932 - val_binary_accuracy: 0.8170\n",
      "Epoch 302/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5930 - binary_accuracy: 0.8166 - val_loss: 0.5928 - val_binary_accuracy: 0.8178\n",
      "Epoch 303/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5929 - binary_accuracy: 0.8198 - val_loss: 0.5923 - val_binary_accuracy: 0.8182\n",
      "Epoch 304/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5923 - binary_accuracy: 0.8178 - val_loss: 0.5919 - val_binary_accuracy: 0.8190\n",
      "Epoch 305/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5916 - binary_accuracy: 0.8196 - val_loss: 0.5914 - val_binary_accuracy: 0.8187\n",
      "Epoch 306/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5912 - binary_accuracy: 0.8197 - val_loss: 0.5909 - val_binary_accuracy: 0.8192\n",
      "Epoch 307/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5908 - binary_accuracy: 0.8196 - val_loss: 0.5905 - val_binary_accuracy: 0.8192\n",
      "Epoch 308/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5905 - binary_accuracy: 0.8202 - val_loss: 0.5900 - val_binary_accuracy: 0.8187\n",
      "Epoch 309/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5897 - binary_accuracy: 0.8199 - val_loss: 0.5896 - val_binary_accuracy: 0.8195\n",
      "Epoch 310/310\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5894 - binary_accuracy: 0.8196 - val_loss: 0.5891 - val_binary_accuracy: 0.8195\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.5954 - binary_accuracy: 0.8170\n",
      "Loss:  0.5953714847564697\n",
      "Accuracy:  0.8169676065444946\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text_  labelzahl\n",
      "0                                                  text_          0\n",
      "1      Love this!  Well made, sturdy, and very comfor...          1\n",
      "2      love it, a great upgrade from the original.  I...          1\n",
      "3      This pillow saved my back. I love the look and...          1\n",
      "4      Missing information on how to use it, but it i...          1\n",
      "...                                                  ...        ...\n",
      "40428  I had read some reviews saying that this bra r...          0\n",
      "40429  I wasn't sure exactly what it would be. It is ...          1\n",
      "40430  You can wear the hood by itself, wear it with ...          0\n",
      "40431  I liked nothing about this dress. The only rea...          1\n",
      "40432  I work in the wedding industry and have to wor...          0\n",
      "\n",
      "[40433 rows x 2 columns]\n",
      "Abschnitt printen\n",
      "                                                text_  labelzahl\n",
      "25  Supposed to come with extra hardware. The only...          1\n",
      "26  Easy to use. Seems to be an easy way to make a...          1\n",
      "27  Not what I am accustomed to. The only reason I...          1\n",
      "28  Nice product & seller. Yes, it is a very thin ...          1\n",
      "29  So good we bought the second set and they look...          1\n",
      "30  Wonderful aroma throughout the house.  The onl...          1\n",
      "31  Received in no time flat.  I love the look.  M...          1\n",
      "32  These are okay they are not as nice as the one...          1\n",
      "33  gave them away - thought it would be a good ad...          1\n",
      "34  Product is fine, wish it had an option to have...          1\n",
      "35  Awesome is all I have to say, the materials ar...          1\n",
      "36  Way too heavy for everyday use. The only reaso...          1\n",
      "37  Well made, works great.  We have had the woode...          1\n",
      "38  Bought these to go with my old suction cups. I...          1\n",
      "39  Just as expected! Looks great and has the desi...          1\n",
      "40  Awful experience, everything stuck, cooked eve...          1\n",
      "41  Does a great job of keeping the suction on. I ...          1\n",
      "42  SHEET COLOR IS NICE BUY FOR MY SIZE FOR MY TOW...          1\n",
      "43  Save $ on these instead of the original shippi...          1\n",
      "44  These towels are real nice, and I love the fee...          1\n",
      "45  Works great, easy to clean, and easy to clean....          1\n",
      "46  Alright for the price,but was a little disappo...          1\n",
      "47  Great quality, beautiful color.  We have had t...          1\n",
      "48  I put paper in one corner of the cabinet and p...          1\n",
      "49  Great stickers but a little small. The only re...          1\n",
      "50  Great quality and the fabric looks great.  It ...          1\n",
      "51  Perfect. They do exactly what I need them to d...          1\n",
      "52  Love this movie & the time it took to finish. ...          1\n",
      "53  Will not seal properly. Atrocious little ones,...          1\n",
      "54  Got these for the third time.  I have a small ...          1\n",
      "55  Excellent product and a much better quality th...          1\n",
      "56  These are just perfect, exactly what I was loo...          0\n",
      "57  Such a great purchase can't beat it for the price          0\n",
      "58  What can you say--- cheap and it works as inte...          0\n",
      "59  These are so nice, sturdy, like the color choi...          0\n",
      "60      It is nice bowl and have had a fast shipping!          0\n",
      "61  Great cup. Will last forever. Keeps things coo...          0\n",
      "62  Love them, just thought they would be a bit bi...          0\n",
      "63  Excellent quality product. Perfect for my ccoz...          0\n",
      "64   This fan is really pretty and I actually use it.          0\n",
      "65  Super rough, not soft wash cloths, more like b...          0\n",
      "66   Like this little guy. Use it often. He is small.          0\n",
      "67  Perfect size .easy to unfold. Topremoves for s...          0\n",
      "68     Does not do a very good job; difficult to use.          0\n",
      "69  frame was beautifully crafted and looks great ...          0\n",
      "70  My hot cocoa is finally perfect! This product ...          0\n",
      "71        i bought this for a friend and he loves it.          0\n",
      "72  Very soft and great quality for such a low price!          0\n",
      "73  Purchased as a Christmas gift.  She will love it.          0\n",
      "74  accurate description, works great, came on tim...          0\n",
      "75  Use it in my Duvet cover. Fluffy and perfect fit!          0\n",
      "76  One of the softest blankets ever. So nice and ...          0\n",
      "77  Smells great, and the fine mist dries quickly....          0\n",
      "78  Exactly what I was expecting. perfect curtains...          0\n",
      "79    Just what I wanted to hang my pants and skirts.          0\n",
      "80  It's kind of small, we were looking for someth...          0\n",
      "81                    A little pricey for what it is.          0\n",
      "82      works great & the charge kept up all day long          0\n",
      "83       This is a nice quality frame at a good price          0\n",
      "84  Does the job, but nothing special. Not hard to...          0\n",
      "predict text printen\n",
      "tf.Tensor(\n",
      "[[1012    7  289 ...    0    0    0]\n",
      " [  79    7   71 ...    0    0    0]\n",
      " [  19   85    3 ...    0    0    0]\n",
      " ...\n",
      " [ 109   23    2 ...    0    0    0]\n",
      " [   9    8    4 ...    0    0    0]\n",
      " [ 119    2  298 ...    0    0    0]], shape=(60, 250), dtype=int64)\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "prediction\n",
      "[[ 0.04731569]\n",
      " [ 0.02875215]\n",
      " [ 0.1273981 ]\n",
      " [ 0.14287204]\n",
      " [ 0.07634071]\n",
      " [ 0.04504311]\n",
      " [ 0.06114754]\n",
      " [-0.01964262]\n",
      " [ 0.11036202]\n",
      " [ 0.09520105]\n",
      " [ 0.12922987]\n",
      " [ 0.16506371]\n",
      " [ 0.16484258]\n",
      " [ 0.02191618]\n",
      " [ 0.05077669]\n",
      " [ 0.04330301]\n",
      " [ 0.04835582]\n",
      " [ 0.08132604]\n",
      " [-0.03607956]\n",
      " [ 0.10623321]\n",
      " [ 0.13447544]\n",
      " [ 0.1386204 ]\n",
      " [ 0.17332146]\n",
      " [-0.02563593]\n",
      " [ 0.18471983]\n",
      " [ 0.10231987]\n",
      " [-0.00502631]\n",
      " [ 0.12771776]\n",
      " [-0.0125303 ]\n",
      " [ 0.10100231]\n",
      " [ 0.0157105 ]\n",
      " [ 0.06323305]\n",
      " [ 0.10577139]\n",
      " [-0.03472713]\n",
      " [ 0.06800017]\n",
      " [ 0.10118958]\n",
      " [ 0.00789979]\n",
      " [ 0.03118074]\n",
      " [ 0.0771572 ]\n",
      " [ 0.11386544]\n",
      " [-0.04457715]\n",
      " [ 0.11437348]\n",
      " [ 0.09187785]\n",
      " [ 0.06767693]\n",
      " [-0.01786944]\n",
      " [ 0.04511419]\n",
      " [ 0.18821415]\n",
      " [ 0.10306814]\n",
      " [ 0.11013341]\n",
      " [ 0.03211984]\n",
      " [ 0.04719779]\n",
      " [-0.00924912]\n",
      " [-0.00435868]\n",
      " [ 0.04326475]\n",
      " [-0.03016421]\n",
      " [ 0.04548347]\n",
      " [ 0.11573717]\n",
      " [ 0.03972253]\n",
      " [ 0.11514282]\n",
      " [ 0.01275724]]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "prediction2\n",
      "[[0.04731566]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Werte größer als 0 sind Fake Reviews'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "#OR = Original reviews (presumably human created and authentic); CG = Computer-generated fake reviews.\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\ma54276\\Desktop\\reviewDetector-main\\python\\fakereviews.csv\",\n",
    "    names=[\"category\",\"rating\",\"label\",\"text_\"])\n",
    "\n",
    "#Electronics_5 = Electronics. Die 5 hat hier keine Bedeutung was die Sterne angeht.\n",
    "df = df[[\"label\",\"text_\"]]\n",
    "\n",
    "#Computer generierte Texte bekommen die Zahl 1 für spam zugeordnet. Im Grunde wird das Label in diesem und\n",
    "#im nächsten Schritt nur als Zahl dargestellt zur einfacheren Verarbeitung.\n",
    "df['labelzahl'] = df['label'].apply(lambda x: 1 if x=='CG' else 0)\n",
    "\n",
    "datenset_xy = df[[\"text_\",\"labelzahl\"]]\n",
    "print(datenset_xy)\n",
    "\n",
    "\n",
    "def get_dataset_partitions_tf(ds=datenset_xy, train_split=0.8, val_split=0.1, test_split=0.1):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "\n",
    "    train_ds=ds.sample(frac=0.8)\n",
    "    val_test_ds=ds.drop(train_ds.index)\n",
    "    val_ds=val_test_ds.sample(frac=0.5)\n",
    "    test_ds=val_test_ds.drop(val_ds.index)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(ds=datenset_xy, train_split=0.8, val_split=0.1, test_split=0.1)\n",
    "\n",
    "\n",
    "train_x = train_ds.pop('text_')\n",
    "train_y = train_ds.pop('labelzahl')\n",
    "\n",
    "val_x = val_ds.pop('text_')\n",
    "val_y = val_ds.pop('labelzahl')\n",
    "\n",
    "test_x = test_ds.pop('text_')\n",
    "test_y = test_ds.pop('labelzahl')\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "\n",
    "# The vocabulary for the layer must be either supplied on construction or learned via adapt().\n",
    "vectorize_layer.adapt(train_x)\n",
    "vectorize_layer.adapt(val_x)\n",
    "vectorize_layer.adapt(test_x)\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(train_x)), next(iter(train_y))\n",
    "print(\"Review\", text_batch)\n",
    "print(\"Label\", label_batch)\n",
    "print(\"Vectorized review\", text_batch, label_batch)\n",
    "\n",
    "print(\" 381 ---> \",vectorize_layer.get_vocabulary()[381])\n",
    "print(\" 133 ---> \",vectorize_layer.get_vocabulary()[133])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n",
    "\n",
    "\n",
    "train_x = vectorize_layer(train_x)\n",
    "val_x = vectorize_layer(val_x)\n",
    "test_x = vectorize_layer(test_x)\n",
    "\n",
    "#################\n",
    "\n",
    "train_x = tf.convert_to_tensor(train_x)\n",
    "train_y = tf.convert_to_tensor(train_y)\n",
    "\n",
    "train_x = tf.cast(train_x, tf.int32)\n",
    "train_y = tf.cast(train_y, tf.int32)\n",
    "\n",
    "train_x = tf.data.Dataset.from_tensors(train_x)\n",
    "train_y = tf.data.Dataset.from_tensors(train_y)\n",
    "\n",
    "################\n",
    "\n",
    "val_x = tf.convert_to_tensor(val_x)\n",
    "val_y = tf.convert_to_tensor(val_y)\n",
    "\n",
    "val_x = tf.cast(val_x, tf.int32)\n",
    "val_y = tf.cast(val_y, tf.int32)\n",
    "\n",
    "val_x = tf.data.Dataset.from_tensors(val_x)\n",
    "val_y = tf.data.Dataset.from_tensors(val_y)\n",
    "\n",
    "#################\n",
    "\n",
    "test_x = tf.convert_to_tensor(test_x)\n",
    "test_y = tf.convert_to_tensor(test_y)\n",
    "\n",
    "test_x = tf.cast(test_x, tf.int32)\n",
    "test_y = tf.cast(test_y, tf.int32)\n",
    "\n",
    "test_x = tf.data.Dataset.from_tensors(test_x)\n",
    "test_y = tf.data.Dataset.from_tensors(test_y)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_x = train_x.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "train_y = train_y.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_x = val_x.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_y = val_y.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_x = test_x.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_y = test_y.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
    "\n",
    "train_data_set = tf.data.Dataset.zip( (train_x , train_y) )\n",
    "val_data_set = tf.data.Dataset.zip( (val_x , val_y) )\n",
    "test_data_set = tf.data.Dataset.zip( (test_x , test_y) )\n",
    "\n",
    "epochs = 310\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_set,\n",
    "    validation_data=val_data_set,\n",
    "    epochs=epochs)\n",
    "\n",
    "model.save('amazon_ein_input.keras')\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data_set)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\ma54276\\Desktop\\reviewDetector-main\\python\\fakereviews.csv\",\n",
    "    names=[\"category\",\"rating\",\"label\",\"text_\"])\n",
    "\n",
    "#Electronics_5 = Electronics. Die 5 hat hier keine Bedeutung was die Sterne angeht.\n",
    "df = df[[\"label\",\"text_\"]]\n",
    "\n",
    "#Computer generierte Texte bekommen die Zahl 1 für spam zugeordnet. Im Grunde wird das Label in diesem und\n",
    "#im nächsten Schritt nur als Zahl dargestellt zur einfacheren Verarbeitung.\n",
    "df['labelzahl'] = df['label'].apply(lambda x: 1 if x=='CG' else 0)\n",
    "\n",
    "datenset_xy = df[[\"text_\",\"labelzahl\"]]\n",
    "print(datenset_xy)\n",
    "\n",
    "print(\"Abschnitt printen\")\n",
    "print(datenset_xy.iloc[25:85])\n",
    "\n",
    "predict_ds = datenset_xy.iloc[25:85]\n",
    "\n",
    "predict_text = predict_ds.pop('text_').to_numpy()\n",
    "\n",
    "predict_text = vectorize_layer(predict_text)\n",
    "\n",
    "print(\"predict text printen\")\n",
    "\n",
    "\n",
    "print(predict_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction = model.predict(predict_text)\n",
    "\n",
    "print(\"prediction\")\n",
    "print(prediction)\n",
    "\n",
    "text = [\"Supposed to come with extra hardware. The only problem is that it's not really a vacuum,\"]\n",
    "\n",
    "text = vectorize_layer(text)\n",
    "\n",
    "prediction2 = model.predict(text)\n",
    "\n",
    "print(\"prediction2\")\n",
    "print(prediction2)\n",
    "\n",
    "\"Werte größer als 0 sind Fake Reviews\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
